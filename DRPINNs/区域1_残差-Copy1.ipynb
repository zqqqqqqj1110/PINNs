{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f14be9e-af19-4021-baeb-3d9035d595e2",
   "metadata": {},
   "source": [
    "## 问题\n",
    "1. 用MSE是否有一些奇怪？（小于0的数更小了）\n",
    "2. 边界采集点会超边界，代码逻辑问题\n",
    "3， 增加pde和ju的权重\n",
    "\n",
    "## 现状\n",
    "1. 边界作为强约束可以给大权重（已解决），但是与此同时pde的值就变小了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b305f8a-ab68-4899-abf8-7e6ff6b81aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "from skopt import gp_minimize\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib as mpl\n",
    "from skopt.utils import use_named_args\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7e577-28e6-4b9b-a1eb-f9c5092974c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 设置GPU设备，如果没有可用的GPU，则使用CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(888888)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b8e44-6caf-4473-8393-3e8454a408eb",
   "metadata": {},
   "source": [
    "## 基础参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8ce7f-11dc-450f-a7e9-debfac3ff856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础参数\n",
    "epochs = 400000    # 训练代数\n",
    "h = 100    # 画图网格密度\n",
    "N = 100    # 内点配置点数\n",
    "N1 = 0.01    # 边界点配置点数\n",
    "n = 1000    # PDE数据点\n",
    "top_k = 100  # 前n个残差最大的点\n",
    "num_new_points = 10  # 以圆心生成的n个点\n",
    "bias = 0.001   # 圆半径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d866751-58b6-41d6-a4a8-888c58e4ef1a",
   "metadata": {},
   "source": [
    "## PINN框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e553fe-dfa8-4d3a-970b-672f0726f5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8d566-c08f-4868-8c02-3691e8a4df6e",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cfe43-6ddc-4c92-b409-1eace5612978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = torch.nn.MSELoss()\n",
    "# loss = torch.nn.L1Loss()  #MAE\n",
    "# loss = torch.nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "# 递归求导\n",
    "def gradients(u, x, order=1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u),\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True, )[0]\n",
    "    else:\n",
    "        return gradients(gradients(u, x), x, order=order - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a7ee9-7b1f-4dde-882e-f9a352258c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = 2 * torch.rand(n, 1, device=device, requires_grad=True) - 1\n",
    "Y = 2 * torch.rand(n, 1, device=device, requires_grad=True) - 1\n",
    "\n",
    "def point(n=100, device=device):\n",
    "    # 随机生成 n 个点\n",
    "    x = X\n",
    "    y = Y\n",
    "    return x, y\n",
    "\n",
    "def l_ugeq(u, n=100):\n",
    "    # u >= 0\n",
    "    x, y = point(n, device)\n",
    "    cond = torch.zeros_like(y)\n",
    "    \n",
    "    # 计算损失\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    llos = torch.relu(-uxy)\n",
    "    ugeq_loss = 100 * torch.sum(llos)\n",
    "    \n",
    "    # 返回每个点的损失值\n",
    "    return ugeq_loss, llos, x, y\n",
    "\n",
    "def l_pde(u, n=100, device=device):\n",
    "    # 等式项损失\n",
    "    x, y = point(n, device)\n",
    "    cond = 2 * torch.pi**2 * torch.sin(torch.pi * x) * torch.sin(torch.pi * y)\n",
    "    \n",
    "    # 计算损失\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    \n",
    "    # 确保梯度计算返回每个点的值\n",
    "    grad_x2 = gradients(uxy, x, 2)  # 对 x 计算二阶梯度\n",
    "    grad_y2 = gradients(uxy, y, 2)  # 对 y 计算二阶梯度\n",
    "\n",
    "    # 计算每个点的损失\n",
    "    loss_value = -grad_x2 - grad_y2  # 假设这里是 PDE 中的计算\n",
    "    loss_value = loss_value - cond   # 计算 PDE 的残差\n",
    "    pde_loss = loss(-gradients(uxy, x, 2) - gradients(uxy, y, 2)- cond, torch.zeros_like(cond))\n",
    "\n",
    "    # 返回每个点的损失值\n",
    "    return pde_loss, loss_value, x, y\n",
    "\n",
    "# def l_JU(u, n=100):\n",
    "#     # 不等式损失\n",
    "#     x, y = point(n, device)\n",
    "#     cond = 2 * torch.pi**2 * torch.sin(torch.pi * x) * torch.sin(torch.pi * y)\n",
    "    \n",
    "#     # 计算损失\n",
    "#     uxy = u(torch.cat([x, y], dim=1))\n",
    "#     llos = 0.5 * (gradients(uxy, x, 1)**2 + gradients(uxy, y, 1)**2) - cond * uxy\n",
    "    \n",
    "#     JU_loss = loss(0.5 * (gradients(uxy, x, 1)**2 + gradients(uxy, y, 1)**2), cond * uxy)\n",
    "    \n",
    "#     # 返回每个点的损失值\n",
    "#     return JU_loss, llos, x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a990a42-5353-4739-b088-bfe0556537f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def l_JU(u, n=100):\n",
    "    # 不等式损失\n",
    "    x, y = point(n, device)  # 随机采样 n 个点\n",
    "    cond = 2 * torch.pi**2 * torch.sin(torch.pi * x) * torch.sin(torch.pi * y)\n",
    "\n",
    "    # 计算 u 的梯度\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    grad_x = gradients(uxy, x, 1)\n",
    "    grad_y = gradients(uxy, y, 1)\n",
    "    \n",
    "    # 计算两个积分项\n",
    "    term1 = 0.5 * (grad_x**2 + grad_y**2)  # 第一个积分项\n",
    "    term2 = cond * uxy  # 第二个积分项\n",
    "\n",
    "    # 近似积分，计算平均值并乘以面积 (这里假设积分域为 [0, 1] x [0, 1])\n",
    "    integral1 = term1.mean()  # 平均值近似第一个积分\n",
    "    integral2 = term2.mean()  # 平均值近似第二个积分\n",
    "\n",
    "    # 总损失\n",
    "    JU_loss = integral1 - integral2  # 对应公式中的两个积分\n",
    "\n",
    "    return JU_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163a3b2-35ed-4cd4-b53d-db50d97d9e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def l_JU(u, n=100, device=device):\n",
    "    \"\"\"\n",
    "    计算每个点的 JU 损失值和整体的 JU 损失（通过蒙特卡罗积分近似）\n",
    "    \n",
    "    参数：\n",
    "    - u: 神经网络\n",
    "    - n: 采样点的数量\n",
    "    - device: 设备（\"cuda\" 或 \"cpu\"）\n",
    "    \n",
    "    返回：\n",
    "    - JU_loss: 总的目标函数值（通过积分近似）\n",
    "    - llos_JU: 每个点的损失值（对应每个采样点）\n",
    "    - x, y: 对应每个点的坐标\n",
    "    \"\"\"\n",
    "    # 随机采样点\n",
    "    x, y = point(n, device)\n",
    "    cond = 2 * torch.pi**2 * torch.sin(torch.pi * x) * torch.sin(torch.pi * y)\n",
    "\n",
    "    # 计算 u 和其梯度\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    grad_x = gradients(uxy, x, 1)\n",
    "    grad_y = gradients(uxy, y, 1)\n",
    "    \n",
    "    # 每个点的损失值\n",
    "    llos_JU = 0.5 * (grad_x**2 + grad_y**2) - cond * uxy  # 每个点对应的损失值\n",
    "    p1 = 0.5 * (grad_x**2 + grad_y**2)\n",
    "    p2 = cond * uxy\n",
    "\n",
    "    # 使用蒙特卡罗积分近似整体损失\n",
    "    JU_loss = llos_JU.mean()  # 取所有点的平均值作为积分的近似\n",
    "\n",
    "    return JU_loss, llos_JU, x, y, p1, p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d759ec-0094-4bd6-8924-5baa39c54fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 边界函数构建\n",
    "def l_boundary1(u):\n",
    "    # 取点\n",
    "    x = torch.arange(0, 1.001, N1, device=device, requires_grad=True).view(-1, 1)\n",
    "    y = torch.ones_like(x, device=device, requires_grad=True)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    # 计算\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "def l_boundary2(u):\n",
    "    # 取点\n",
    "    y = torch.arange(0, 1.001, N1, device=device, requires_grad=True).view(-1, 1)\n",
    "    x = torch.ones_like(y, device=device, requires_grad=True)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    # 计算\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "def l_boundary3(u):\n",
    "    # 取点\n",
    "    y = torch.arange(0, 1.001, N1, device=device, requires_grad=True).view(-1, 1)\n",
    "    x = torch.zeros_like(y, device=device, requires_grad=True)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    # 计算\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "def l_boundary4(u):\n",
    "    # 取点\n",
    "    x = torch.arange(0, 1.001, N1, device=device, requires_grad=True).view(-1, 1)\n",
    "    y = torch.zeros_like(x, device=device, requires_grad=True)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    # 计算\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b81811-e314-426a-b798-a2c23a816455",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 数据集更新 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d07adb-453e-464b-8b37-973fb341941b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_dataset(X, Y, inside_loss, x_pde, y_pde, top_k=10, num_new_points=10, bias=0.001, device=device):\n",
    "    \"\"\"\n",
    "    更新数据集，包括：\n",
    "    - 找到前 top_k 个高损失点\n",
    "    - 以高损失点为圆心生成随机新点\n",
    "    - 删除高损失点\n",
    "    - 去重数据集\n",
    "    \n",
    "    参数:\n",
    "    X (Tensor): 当前数据集的 X 坐标\n",
    "    Y (Tensor): 当前数据集的 Y 坐标\n",
    "    inside_loss (Tensor): 每个点的损失值\n",
    "    x_pde (Tensor): PDE 数据的 X 坐标\n",
    "    y_pde (Tensor): PDE 数据的 Y 坐标\n",
    "    top_k (int): 选择前 top_k 个高损失点\n",
    "    num_new_points (int): 每个高损失点生成的新点数\n",
    "    bias (float): 新点生成的半径\n",
    "    device (str): 设备类型 (\"cpu\" 或 \"cuda\")\n",
    "\n",
    "    返回:\n",
    "    X (Tensor): 更新后的 X 坐标\n",
    "    Y (Tensor): 更新后的 Y 坐标\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        # 获取绝对损失值\n",
    "        abs_loss = torch.abs(inside_loss)  # 对每个点的损失取绝对值\n",
    "\n",
    "        # 将损失值和对应的 x, y 坐标组合为一个元组 (loss, x, y)\n",
    "        loss_coords = list(zip(abs_loss.cpu().numpy(), x_pde.cpu().numpy(), y_pde.cpu().numpy()))\n",
    "\n",
    "        # 按照损失值进行排序（按损失的绝对值降序）\n",
    "        sorted_loss_coords = sorted(loss_coords, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # 获取前 top_k 个高损失点\n",
    "        top_k_x = [sorted_loss_coords[i][1] for i in range(top_k)]\n",
    "        top_k_y = [sorted_loss_coords[i][2] for i in range(top_k)]\n",
    "        \n",
    "        top_k_x_tensor = torch.tensor(top_k_x, device=device).view(-1, 1)\n",
    "        top_k_y_tensor = torch.tensor(top_k_y, device=device).view(-1, 1)\n",
    "\n",
    "        # 从 X 和 Y 中删除这些点\n",
    "        mask = torch.ones(X.shape[0], dtype=torch.bool, device=device)\n",
    "        for i in range(top_k):\n",
    "            mask &= ~((X == top_k_x_tensor[i]).view(-1) & (Y == top_k_y_tensor[i]).view(-1))\n",
    "        X = X[mask]\n",
    "        Y = Y[mask]\n",
    "\n",
    "        # 为每个点生成附近的随机点\n",
    "        all_new_x = []\n",
    "        all_new_y = []\n",
    "        for i in range(top_k):\n",
    "            center_x = top_k_x_tensor[i]\n",
    "            center_y = top_k_y_tensor[i]\n",
    "\n",
    "            # 随机生成 n 个附近点\n",
    "            angles = torch.rand(num_new_points, device=device) * 2 * torch.pi\n",
    "            radii = torch.sqrt(torch.rand(num_new_points, device=device)) * bias\n",
    "            offset_x = radii * torch.cos(angles)\n",
    "            offset_y = radii * torch.sin(angles)\n",
    "\n",
    "            new_points_x = center_x + offset_x\n",
    "            new_points_y = center_y + offset_y\n",
    "\n",
    "            all_new_x.append(new_points_x)\n",
    "            all_new_y.append(new_points_y)\n",
    "\n",
    "        # 将所有新点的 x 和 y 坐标合并\n",
    "        all_new_x = torch.cat(all_new_x, dim=0).view(-1, 1)\n",
    "        all_new_y = torch.cat(all_new_y, dim=0).view(-1, 1)\n",
    "\n",
    "        # 将新点添加回数据集\n",
    "        X = torch.cat([X, all_new_x], dim=0)\n",
    "        Y = torch.cat([Y, all_new_y], dim=0)\n",
    "\n",
    "        # 去除重复点\n",
    "        points = torch.cat([X, Y], dim=1)  # 合并为 (x, y) 点对\n",
    "        points = torch.unique(points, dim=0)  # 按行去重\n",
    "        X, Y = points[:, 0].view(-1, 1), points[:, 1].view(-1, 1)  # 拆分回 X 和 Y\n",
    "\n",
    "    # 返回时重新启用 requires_grad\n",
    "    return X.requires_grad_(True), Y.requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907805a-8fa0-4842-91ed-c349a0770363",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 查看情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc4c22-a82b-46d2-a9de-94f06d06a72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def record_loss_and_save_model(u, epoch, maxerror, current_time, h, device):\n",
    "    \"\"\"\n",
    "    每100次迭代记录一次损失并保存模型。\n",
    "\n",
    "    参数:\n",
    "    - u: 模型\n",
    "    - epoch: 当前训练的 epoch 数\n",
    "    - maxerror: 当前最小的误差\n",
    "    - current_time: 上一次记录的时间\n",
    "    - h: 网格划分大小\n",
    "    - device: 设备\n",
    "    返回:\n",
    "    - maxerror: 更新后的最小误差\n",
    "    - current_time: 更新后的时间\n",
    "    \"\"\"\n",
    "    # 生成网格点\n",
    "    xc_x = torch.linspace(0, 1, h, device=device)\n",
    "    xc_y = torch.linspace(0, 1, h, device=device)\n",
    "    xm, ym = torch.meshgrid(xc_x, xc_y)\n",
    "    xx = xm.reshape(-1, 1)\n",
    "    yy = ym.reshape(-1, 1)\n",
    "    xy = torch.cat([xx, yy], dim=1).to(device)\n",
    "\n",
    "    # 计算预测值和真实值\n",
    "    u_pred = u(xy)\n",
    "    u_real = torch.relu(torch.sin(torch.pi * xx) * torch.sin(torch.pi * yy))\n",
    "    u_error = torch.abs(u_pred - u_real)\n",
    "\n",
    "    # 计算误差网格\n",
    "    u_pred_fig = u_pred.reshape(h, h)\n",
    "    u_real_fig = u_real.reshape(h, h)\n",
    "    u_error_fig = u_error.reshape(h, h)\n",
    "\n",
    "    # 计算当前最大绝对误差\n",
    "    max_abs_error = float(torch.max(u_error))\n",
    "    print(f\"At epoch {epoch}, time_speed: {abs(current_time - time.time()):.2f}s, Max abs error is: {max_abs_error}, best: {maxerror}\")\n",
    "    print('-----------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    # 如果误差更小，则保存模型\n",
    "    if max_abs_error < maxerror:\n",
    "        maxerror = max_abs_error\n",
    "        torch.save(u.state_dict(), 'weights11.pth')\n",
    "\n",
    "    # 更新当前时间\n",
    "    current_time = time.time()\n",
    "\n",
    "    return maxerror, current_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c826b5-1a00-4895-9e0a-425426b08f87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399226c0-ea52-40b8-9afe-e69aad6ef44d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m\n\u001b[1;32m     35\u001b[0m     maxerror, current_time \u001b[38;5;241m=\u001b[39m record_loss_and_save_model(u, epoch, maxerror, current_time, h, device)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# if epoch % 100000 == 0 and epoch != 0:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     X, Y = update_dataset(X, Y, inside_loss, x_pde, y_pde, top_k, num_new_points, bias, device)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Training\n",
    "u = MLP().to(device)\n",
    "opt = torch.optim.Adam(params=u.parameters(), lr=0.0001)\n",
    "\n",
    "# 设置初始权重\n",
    "weight_pde = 60000\n",
    "weight_JU = 200\n",
    "weight_boundary = 10000000\n",
    "current_time = time.time()\n",
    "maxerror = 99999999999\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # # 手动调整学习率\n",
    "    # if epoch == 100000:\n",
    "    #     for param_group in opt.param_groups:\n",
    "    #         param_group['lr'] *= 5  # 学习率放大 1/10 倍\n",
    "    \n",
    "    # 计算各个损失函数并获取点\n",
    "    pde_loss, llos_pde, x_pde, y_pde = l_pde(u)\n",
    "    JU_loss, llos_JU, x_JU, y_JU, p1, p2 = l_JU(u)\n",
    "    ugeq_loss, llos_ugeq, x_ugeq, y_ugeq = l_ugeq(u)\n",
    "    \n",
    "    inside_loss = weight_pde * llos_pde + weight_JU * llos_JU + 6000 * llos_ugeq\n",
    "    loss_boundary = l_boundary1(u) +  l_boundary2(u) + l_boundary3(u) + l_boundary4(u)\n",
    "    \n",
    "    # 计算加权总损失\n",
    "    total_loss = weight_pde * pde_loss + weight_JU * JU_loss + 600 * ugeq_loss + weight_boundary * loss_boundary\n",
    "    # 每100次迭代记录一次损失\n",
    "    if epoch % 100 == 0 and epoch != 0:\n",
    "        print(f\"pde:{pde_loss.item()},JU:{JU_loss.item()},boundary:{loss_boundary.item()}\")\n",
    "        print(p1.mean().item(), p2.mean().item())\n",
    "        maxerror, current_time = record_loss_and_save_model(u, epoch, maxerror, current_time, h, device)\n",
    "        \n",
    "    # if epoch % 100000 == 0 and epoch != 0:\n",
    "    #     X, Y = update_dataset(X, Y, inside_loss, x_pde, y_pde, top_k, num_new_points, bias, device)\n",
    "    \n",
    "    # 反向传播和优化\n",
    "    total_loss.backward()\n",
    "    opt.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22d09c2a-850d-4306-85fa-ec5ee44955d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0040, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(l_boundary1(u))\n",
    "print(l_boundary2(u))\n",
    "print(l_boundary3(u))\n",
    "print(l_boundary4(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db6d6a-fafd-4b89-a5e3-bab8237f6d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
