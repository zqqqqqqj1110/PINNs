{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e619d-4f5f-4264-ad47-79e3f9df606e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "from skopt import gp_minimize\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib as mpl\n",
    "from skopt.utils import use_named_args\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e433891-66aa-481c-8385-a315dba9ff84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 设置GPU设备，如果没有可用的GPU，则使用CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(888888)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7cbc0a-e140-4a22-a758-c3479e802987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础参数\n",
    "epochs = 50001    # 训练代数\n",
    "h = 1000    # 画图网格密度\n",
    "N = 1000    # 内点配置点数\n",
    "N1 = 0.01    # 边界点配置点数\n",
    "N2 = 1000    # PDE数据点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b56279-9949-493c-a38e-dd4a7d8e413f",
   "metadata": {},
   "source": [
    "# 约束函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f83e07-b247-4863-916e-c80af07b4c43",
   "metadata": {},
   "source": [
    "## 内点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68976f6-f62d-47be-938e-4e74dec79e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ugeq(n=N2):\n",
    "    x = torch.rand(n, 1, device=device)\n",
    "    y = torch.rand(n, 1, device=device)\n",
    "    cond = torch.zeros_like(y)\n",
    "    return x.requires_grad_(True), y.requires_grad_(True), cond\n",
    "\n",
    "def pde(n=N2):\n",
    "    x = torch.rand(n, 1, device=device)\n",
    "    y = torch.rand(n, 1, device=device)\n",
    "    cond = 2 * torch.pi**2 * torch.sin(torch.pi * x) * torch.sin(torch.pi * y)\n",
    "    return x.requires_grad_(True), y.requires_grad_(True), cond\n",
    "\n",
    "def JU(n=N2):\n",
    "    # 不等式形\n",
    "    x = torch.rand(n, 1, device=device)\n",
    "    y = torch.rand(n, 1, device=device)\n",
    "    cond = 2 * torch.pi**2 * torch.sin(torch.pi * x) * torch.sin(torch.pi * y)\n",
    "    return x.requires_grad_(True), y.requires_grad_(True), cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f6e0f-8aad-4feb-a012-f9d614f56b67",
   "metadata": {},
   "source": [
    "## 边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14bb532-1d67-4539-b1c7-df3f430ec0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 边界条件\n",
    "def boundary1(n=N1):    # 上边界\n",
    "    x = torch.arange(0, 1.001, N1, device=device).view(-1, 1)\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    return x.requires_grad_(True), y.requires_grad_(True), cond\n",
    "\n",
    "def boundary2(n=N1):    # 右边界\n",
    "    y = torch.arange(0, 1.001, N1, device=device).view(-1, 1)\n",
    "    x = torch.ones_like(y, device=device)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    return x.requires_grad_(True), y.requires_grad_(True), cond\n",
    "\n",
    "def boundary3(n=N1):\n",
    "    # y轴\n",
    "    y = torch.arange(0, 1.001, N1, device=device).view(-1, 1)\n",
    "    x = torch.zeros_like(y, device=device)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    return x.requires_grad_(True), y.requires_grad_(True), cond\n",
    "\n",
    "def boundary4(n=N1):\n",
    "    # x轴\n",
    "    x = torch.arange(0, 1.001, N1, device=device).view(-1, 1)\n",
    "    y = torch.zeros_like(x, device=device)\n",
    "    cond = torch.zeros_like(x, device=device)\n",
    "    return x.requires_grad_(True), y.requires_grad_(True), cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454b9d0-b316-448f-a497-52534cbcf193",
   "metadata": {},
   "source": [
    "# Plnn框架\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e1c77e-f7f6-4cf0-9bbd-f00651e195e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818e909-9ab6-48a8-bd88-71dd08a6bfea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ecedda-4111-42bf-941b-c0e52c6154c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# 递归求导\n",
    "def gradients(u, x, order=1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u),\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True, )[0]\n",
    "    else:\n",
    "        return gradients(gradients(u, x), x, order=order - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca75379-dd77-4f9e-98a7-adbf58d49d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def l_ugeq(u):\n",
    "    # u>=0\n",
    "    x, y, cond = ugeq()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    llos = torch.relu(-uxy)\n",
    "    return 100 * torch.sum(llos)\n",
    "\n",
    "def l_pde(u):\n",
    "    # 等式项损失\n",
    "    x, y, cond = pde()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(-gradients(uxy, x, 2) - gradients(uxy, y, 2), cond)\n",
    "\n",
    "def l_JU(u):\n",
    "    # 不等式损失\n",
    "    x, y, cond = JU()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    llos = 0.5 * (gradients(uxy, x, 1)**2 + gradients(uxy, y, 1)**2) - cond * uxy\n",
    "    return loss(0.5 * (gradients(uxy, x, 1)**2 + gradients(uxy, y, 1)**2), cond * uxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b7632f-0356-433f-906a-cabd99e9049b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 边界函数构建\n",
    "def l_boundary1(u):\n",
    "    x, y, cond = boundary1()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "def l_boundary2(u):\n",
    "    x, y, cond = boundary2()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "def l_boundary3(u):\n",
    "    x, y, cond = boundary3()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "def l_boundary4(u):\n",
    "    x, y, cond = boundary4()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26170dde-69a8-478d-93ea-87ab60254e1a",
   "metadata": {},
   "source": [
    "# 纯训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af16fd49-c3f1-4637-8d94-a9d525e1f77e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, time_speed:0.5833084583282471 loss is: 19608414.0\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss_pde \u001b[38;5;241m=\u001b[39m l_pde(u)\n\u001b[0;32m---> 17\u001b[0m loss_JU \u001b[38;5;241m=\u001b[39m \u001b[43ml_JU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss_ugeq \u001b[38;5;241m=\u001b[39m l_ugeq(u)\n\u001b[1;32m     19\u001b[0m loss_boundary \u001b[38;5;241m=\u001b[39m l_boundary1(u) \u001b[38;5;241m+\u001b[39m  l_boundary2(u) \u001b[38;5;241m+\u001b[39m l_boundary3(u) \u001b[38;5;241m+\u001b[39m l_boundary4(u)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36ml_JU\u001b[0;34m(u)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ml_JU\u001b[39m(u):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 不等式损失\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x, y, cond \u001b[38;5;241m=\u001b[39m JU()\n\u001b[0;32m---> 17\u001b[0m     uxy \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     llos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (gradients(uxy, x, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m gradients(uxy, y, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m cond \u001b[38;5;241m*\u001b[39m uxy\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (gradients(uxy, x, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m gradients(uxy, y, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m), cond \u001b[38;5;241m*\u001b[39m uxy)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "u = MLP().to(device)\n",
    "opt = torch.optim.Adam(params=u.parameters(), lr=0.0005)\n",
    "current_time = time.time()\n",
    "\n",
    "# 设置初始权重\n",
    "weight_pde = 55\n",
    "weight_JU = 2\n",
    "weight_boundary = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    opt.zero_grad()\n",
    "\n",
    "    # 计算损失\n",
    "    loss_pde = l_pde(u)\n",
    "    loss_JU = l_JU(u)\n",
    "    loss_ugeq = l_ugeq(u)\n",
    "    loss_boundary = l_boundary1(u) +  l_boundary2(u) + l_boundary3(u) + l_boundary4(u)\n",
    "\n",
    "    # 计算加权总损失\n",
    "    total_loss = weight_pde * loss_pde + weight_JU * loss_JU + 1000 * loss_ugeq + weight_boundary * loss_boundary\n",
    "\n",
    "    # 反向传播和优化\n",
    "    total_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # 每100次迭代记录一次损失\n",
    "    if i % 100 == 0:\n",
    "        print(f\"At epoch {i}, time_speed:{abs(current_time-time.time())} loss is: {total_loss.data}\")\n",
    "        print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        current_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0b5ee-a3f0-46d8-bba0-79b3772d84e0",
   "metadata": {},
   "source": [
    "# 贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd086d2-4920-414c-b453-24bd26021851",
   "metadata": {},
   "source": [
    "## 权重训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb4e3a-a2ce-48de-8134-53d2c76819b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train_model(weight_pde, weight_JU, weight_boundary):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    u = MLP().to(device)\n",
    "    opt = torch.optim.Adam(params=u.parameters(), lr=0.0005)\n",
    "    current_time = time.time()\n",
    "    epochs = 1000  # 设置为实际的训练轮数\n",
    "    for i in range(epochs):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # 计算损失\n",
    "        loss_pde = l_pde(u)\n",
    "        loss_JU = l_JU(u)\n",
    "        loss_ugeq = l_ugeq(u)\n",
    "        loss_boundary = l_boundary1(u) + l_boundary2(u) + l_boundary3(u) + l_boundary4(u)\n",
    "        \n",
    "        # 计算加权总损失\n",
    "        total_loss = weight_pde * loss_pde + weight_JU * loss_JU + 1000 * loss_ugeq + weight_boundary * loss_boundary\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # 每100次迭代记录一次损失\n",
    "        if i % 100 == 0:\n",
    "            print(f\"At epoch {i}, time_speed:{abs(current_time-time.time())} loss is: {total_loss.data}\")\n",
    "            print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "            current_time = time.time()\n",
    "    \n",
    "    # 返回最终的损失作为优化目标\n",
    "    return total_loss.item()\n",
    "\n",
    "# 定义贝叶斯优化目标函数\n",
    "def objective(trial):\n",
    "    weight_pde = trial.suggest_float('weight_pde', 1, 100)\n",
    "    weight_JU = trial.suggest_float('weight_JU', 1, 100)\n",
    "    weight_boundary = trial.suggest_float('weight_boundary', 1, 100)\n",
    "    return train_model(weight_pde, weight_JU, weight_boundary)\n",
    "\n",
    "# 运行贝叶斯优化\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 输出最佳权重\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686fd6b-95c5-4b1b-b5b5-6cc16de94b90",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337b85b-764e-4597-a4c8-42aeada053f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials = [\n",
    "    {'weight_pde': 55.0, 'weight_JU': 2.0, 'weight_boundary': 100.0, 'total_loss': 0.123},\n",
    "    {'weight_pde': 60.5, 'weight_JU': 3.1, 'weight_boundary': 95.2, 'total_loss': 0.117},\n",
    "    {'weight_pde': 50.2, 'weight_JU': 1.8, 'weight_boundary': 105.3, 'total_loss': 0.119},\n",
    "    # 更多试验数据...\n",
    "]\n",
    "\n",
    "# 提取数据\n",
    "weights_pde = [trial['weight_pde'] for trial in trials]\n",
    "weights_JU = [trial['weight_JU'] for trial in trials]\n",
    "weights_boundary = [trial['weight_boundary'] for trial in trials]\n",
    "total_losses = [trial['total_loss'] for trial in trials]\n",
    "trial_numbers = list(range(1, len(trials) + 1))\n",
    "\n",
    "# 绘制权重变化折线图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trial_numbers, weights_pde, label='Weight PDE')\n",
    "plt.plot(trial_numbers, weights_JU, label='Weight JU')\n",
    "plt.plot(trial_numbers, weights_boundary, label='Weight Boundary')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.title('Weight Values Across Trials')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制总损失变化折线图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trial_numbers, total_losses, label='Total Loss', color='red')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss Across Trials')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66420d-d259-4c80-8ab2-2f49e2b810e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
