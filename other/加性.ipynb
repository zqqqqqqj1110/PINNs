{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7f86a7-685a-498a-b936-567809e9a28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee26be2-090b-4f01-99dd-c24901f3bf6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "No GPU available, using CPU\n"
     ]
    }
   ],
   "source": [
    "# 设置GPU设备，如果没有可用的GPU，则使用CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(888888)\n",
    "\n",
    "print(device)\n",
    "\n",
    "# 打印GPU型号\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f'Number of available GPUs: {num_gpus}')\n",
    "    for i in range(num_gpus):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "else:\n",
    "    print('No GPU available, using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2904d5b3-89c7-4027-9131-75844df861f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223bac68-ce90-4054-a94a-b4a85b0f5fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础参数\n",
    "epochs = 201    # 训练代数\n",
    "N = 100    # 内点配置点数\n",
    "N1 = 0.1    # 边界点配置点数\n",
    "N2 = 1000    # PDE数据点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7290ff-7105-486c-88a9-e936d1373458",
   "metadata": {},
   "source": [
    "# 需要改的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e609a6-5e47-4f10-be1c-0c5518824988",
   "metadata": {},
   "source": [
    "20 50 100 200 500 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9259f0a9-35df-4f6e-b823-aa47720709a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 论文参数\n",
    "h = 200      # 网格分辨率1/h=0.05\n",
    "delta = 0.002   # 小区域的δ值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebac02-4f50-41cb-95df-7ae08dbbb42a",
   "metadata": {},
   "source": [
    "# 迭代步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df4cee3-fd55-41b1-984b-16dced87e94b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weights1_+.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m MLP()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 加载权重\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model1\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights1_+.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m model2\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights2_+.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 输入数据--右\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights1_+.pth'"
     ]
    }
   ],
   "source": [
    "# 初始化两个模型\n",
    "model1 = MLP().to(device)\n",
    "model2 = MLP().to(device)\n",
    "\n",
    "# 加载权重\n",
    "model1.load_state_dict(torch.load('weights1_+.pth', map_location=device))\n",
    "model2.load_state_dict(torch.load('weights2_+.pth', map_location=device))\n",
    "\n",
    "# 输入数据--右\n",
    "x = delta\n",
    "y = torch.linspace(0, 1, steps=h).to(device)  # 这里选择h个点，可以根据需要调整\n",
    "input_data_right = torch.stack([torch.full_like(y, x), y], dim=1)\n",
    "\n",
    "# 输入数据--左\n",
    "x = -delta\n",
    "y = torch.linspace(0, 1, steps=h).to(device)  # 选择h个点\n",
    "input_data_left = torch.stack([torch.full_like(y, x), y], dim=1)\n",
    "\n",
    "input_data_left = input_data_left.to(device)\n",
    "input_data_right = input_data_right.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f6d3f01d-c20b-4cce-83ee-2770e77364fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代，结果：102.66362762451172\n",
      "-----------------------------------------------------------------------------\n",
      "第2次迭代，结果：54.145301818847656\n",
      "-----------------------------------------------------------------------------\n",
      "第3次迭代，结果：30.5867919921875\n",
      "-----------------------------------------------------------------------------\n",
      "第4次迭代，结果：19.653488159179688\n",
      "-----------------------------------------------------------------------------\n",
      "第5次迭代，结果：14.147781372070312\n",
      "-----------------------------------------------------------------------------\n",
      "第6次迭代，结果：10.817544937133789\n",
      "-----------------------------------------------------------------------------\n",
      "第7次迭代，结果：8.481889724731445\n",
      "-----------------------------------------------------------------------------\n",
      "第8次迭代，结果：6.7563018798828125\n",
      "-----------------------------------------------------------------------------\n",
      "第9次迭代，结果：5.400205135345459\n",
      "-----------------------------------------------------------------------------\n",
      "第10次迭代，结果：4.310580730438232\n",
      "-----------------------------------------------------------------------------\n",
      "第11次迭代，结果：3.4662795066833496\n",
      "-----------------------------------------------------------------------------\n",
      "第12次迭代，结果：2.7939765453338623\n",
      "-----------------------------------------------------------------------------\n",
      "第13次迭代，结果：2.302891492843628\n",
      "-----------------------------------------------------------------------------\n",
      "第14次迭代，结果：1.9461965560913086\n",
      "-----------------------------------------------------------------------------\n",
      "第15次迭代，结果：1.6721014976501465\n",
      "-----------------------------------------------------------------------------\n",
      "第16次迭代，结果：1.4607510566711426\n",
      "-----------------------------------------------------------------------------\n",
      "第17次迭代，结果：1.2855829000473022\n",
      "-----------------------------------------------------------------------------\n",
      "第18次迭代，结果：1.1452280282974243\n",
      "-----------------------------------------------------------------------------\n",
      "第19次迭代，结果：1.0266841650009155\n",
      "-----------------------------------------------------------------------------\n",
      "第20次迭代，结果：0.9366471171379089\n",
      "-----------------------------------------------------------------------------\n",
      "第21次迭代，结果：0.8474372029304504\n",
      "-----------------------------------------------------------------------------\n",
      "第22次迭代，结果：0.7728210091590881\n",
      "-----------------------------------------------------------------------------\n",
      "第23次迭代，结果：0.698857843875885\n",
      "-----------------------------------------------------------------------------\n",
      "第24次迭代，结果：0.6387745141983032\n",
      "-----------------------------------------------------------------------------\n",
      "第25次迭代，结果：0.5872573852539062\n",
      "-----------------------------------------------------------------------------\n",
      "第26次迭代，结果：0.5438520312309265\n",
      "-----------------------------------------------------------------------------\n",
      "第27次迭代，结果：0.49193862080574036\n",
      "-----------------------------------------------------------------------------\n",
      "第28次迭代，结果：0.4613665044307709\n",
      "-----------------------------------------------------------------------------\n",
      "第29次迭代，结果：0.4254530668258667\n",
      "-----------------------------------------------------------------------------\n",
      "第30次迭代，结果：0.39640530943870544\n",
      "-----------------------------------------------------------------------------\n",
      "第31次迭代，结果：0.3702199459075928\n",
      "-----------------------------------------------------------------------------\n",
      "第32次迭代，结果：0.3521438241004944\n",
      "-----------------------------------------------------------------------------\n",
      "第33次迭代，结果：0.3244337737560272\n",
      "-----------------------------------------------------------------------------\n",
      "第34次迭代，结果：0.3121781349182129\n",
      "-----------------------------------------------------------------------------\n",
      "第35次迭代，结果：0.29292207956314087\n",
      "-----------------------------------------------------------------------------\n",
      "第36次迭代，结果：0.28402405977249146\n",
      "-----------------------------------------------------------------------------\n",
      "第37次迭代，结果：0.26359477639198303\n",
      "-----------------------------------------------------------------------------\n",
      "第38次迭代，结果：0.25827497243881226\n",
      "-----------------------------------------------------------------------------\n",
      "第39次迭代，结果：0.24470020830631256\n",
      "-----------------------------------------------------------------------------\n",
      "第40次迭代，结果：0.24195025861263275\n",
      "-----------------------------------------------------------------------------\n",
      "第41次迭代，结果：0.2279604822397232\n",
      "-----------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m model1\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_right\u001b[49m\u001b[43m)\u001b[49m               \n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(output \u001b[38;5;241m-\u001b[39m output2))  \u001b[38;5;66;03m# 重新计算损失\u001b[39;00m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 添加n次的平均迭代次数与时间，改Adam\n",
    "ave_count = ave_time = 0\n",
    "min_count = min_time = 99999\n",
    "current_time = time.time()\n",
    "c = 0\n",
    "for i in range(1):\n",
    "    c += 1\n",
    "    count = 0      # 迭代计数器\n",
    "    # 重新导入\n",
    "    model1.load_state_dict(torch.load('weights1.pth', map_location=device))\n",
    "    model2.load_state_dict(torch.load('weights2.pth', map_location=device))\n",
    "    while 1:\n",
    "        # 记录迭代次数\n",
    "        count += 1\n",
    "\n",
    "        # 计算左右两直线\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        with torch.no_grad():\n",
    "            output1 = model1(input_data_left)             # 左直线\n",
    "            output2 = model2(input_data_right)            # 右直线\n",
    "\n",
    "\n",
    "        # 进行对区域1的训练\n",
    "        optimizer = optim.Adam(model1.parameters(), lr=0.00001)\n",
    "        for epoch in range(epochs):\n",
    "            model1.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model1(input_data_right)               \n",
    "            loss = torch.sum(torch.abs(output - output2))  # 重新计算损失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        # 进行对区域2的训练\n",
    "        optimizer = optim.Adam(model2.parameters(), lr=0.00001)\n",
    "        for epoch in range(epochs):\n",
    "            model2.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model2(input_data_left)                           # 重新计算output2\n",
    "            loss = torch.sum(torch.abs(output1 - output))              # 重新计算损失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # 退出操作\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        with torch.no_grad():\n",
    "            result_left = model1(input_data_left)\n",
    "            result_right = model2(input_data_right)\n",
    "        result = torch.abs(torch.sum(result_left - result_right))\n",
    "        # 收敛退出循环\n",
    "        if (result / count) < 1e-3:\n",
    "            T = abs(current_time-time.time())\n",
    "            print(f'迭代结束!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!次数为{count},时间为：{T}s当前结果为：{result / count}')\n",
    "            current_time = time.time()\n",
    "            break\n",
    "        # 隔10次打印结果\n",
    "        elif count % 1 == 0:\n",
    "            print(f'第{count}次迭代，结果：{result / count}')\n",
    "            # print(result_left.T - result_right.T)\n",
    "            print('-----------------------------------------------------------------------------')\n",
    "\n",
    "    if min_count > count and count != 0:\n",
    "        min_count = count\n",
    "    if min_time > T and count != 0:\n",
    "        min_time = T\n",
    "        \n",
    "    ave_time += T\n",
    "    ave_count += count\n",
    "    print('***************************************************************************')\n",
    "\n",
    "# 最终结果\n",
    "print(f'迭代次数：{min_count},使用时间：{min_time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb009b60-3111-4eef-b706-ab6bae112ad4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
